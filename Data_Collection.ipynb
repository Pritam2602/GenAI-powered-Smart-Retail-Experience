{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a13bd82-2d0e-4f1f-814c-f5cc7adcbe04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GenAI-powered Smart Retail Experience\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\GenAI-powered Smart Retail Experience\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0472b45f-18ce-4d52-a584-532685dcd2d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: requests in d:\\anaconda3\\lib\\site-packages (2.32.5)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda3\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3\\lib\\site-packages (2.3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in d:\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\anaconda3\\lib\\site-packages (from requests) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\anaconda3\\lib\\site-packages (from requests) (2025.10.5)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in d:\\anaconda3\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.2.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests beautifulsoup4 pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2117bf3f-b490-4d8d-8201-0cd8a1675615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==============================\n",
      "üîπ SCRAPING CATEGORY: MEN+T+SHIRTS\n",
      "==============================\n",
      "\n",
      "‚û° Page 1\n",
      "\n",
      "‚û° Page 2\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "\n",
    "# üóÇ Create folder for all images\n",
    "os.makedirs(\"fashion_images\", exist_ok=True)\n",
    "\n",
    "# üîç Fashion categories to scrape\n",
    "queries = [\n",
    "    \"men+t+shirts\",\n",
    "    \"men+jeans\",\n",
    "    \"men+shoes\",\n",
    "    \"women+dresses\",\n",
    "    \"women+tops\",\n",
    "    \"women+sarees\",\n",
    "    \"women+handbags\",\n",
    "    \"unisex+sneakers\",\n",
    "    \"kids+clothes\"\n",
    "]\n",
    "\n",
    "# üßæ List to hold all data\n",
    "all_data = []\n",
    "\n",
    "# üåç Loop through each category\n",
    "for query in queries:\n",
    "    base_url = f\"https://www.flipkart.com/search?q={query}\"\n",
    "    print(f\"\\n==============================\")\n",
    "    print(f\"üîπ SCRAPING CATEGORY: {query.upper()}\")\n",
    "    print(f\"==============================\")\n",
    "\n",
    "    for page in range(1, 3):  # scrape first 2 pages per category\n",
    "        print(f\"\\n‚û° Page {page}\")\n",
    "        url = base_url + f\"&page={page}\"\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # collect all product page links\n",
    "        product_links = []\n",
    "        for link in soup.find_all(\"a\", {\"class\": \"IRpwTa\"}):\n",
    "            product_links.append(\"https://www.flipkart.com\" + link['href'])\n",
    "        for link in soup.find_all(\"a\", {\"class\": \"WKTcLC\"}):\n",
    "            product_links.append(\"https://www.flipkart.com\" + link['href'])\n",
    "\n",
    "        for product_url in product_links:\n",
    "            try:\n",
    "                res = requests.get(product_url)\n",
    "                psoup = BeautifulSoup(res.text, 'html.parser')\n",
    "                \n",
    "                # name\n",
    "                try:\n",
    "                    name = psoup.find(\"span\", {\"class\": \"B_NuCI\"}).text.strip()\n",
    "                except:\n",
    "                    name = \"Unknown\"\n",
    "                \n",
    "                # brand\n",
    "                brand = name.split()[0] if name != \"Unknown\" else \"Unknown\"\n",
    "                \n",
    "                # price\n",
    "                try:\n",
    "                    price = psoup.find(\"div\", {\"class\": \"_30jeq3 _16Jk6d\"}).text.replace(\"‚Çπ\", \"\").replace(\",\", \"\")\n",
    "                except:\n",
    "                    price = None\n",
    "                \n",
    "                # rating\n",
    "                try:\n",
    "                    rating = psoup.find(\"div\", {\"class\": \"_3LWZlK\"}).text\n",
    "                except:\n",
    "                    rating = None\n",
    "                \n",
    "                # description\n",
    "                try:\n",
    "                    highlights = psoup.find_all(\"li\", {\"class\": \"_21Ahn-\"})\n",
    "                    description = \", \".join([h.text for h in highlights])\n",
    "                except:\n",
    "                    description = \"No description\"\n",
    "                \n",
    "                # image\n",
    "                try:\n",
    "                    image_tag = psoup.find(\"img\", {\"class\": \"_396cs4 _2amPTt _3qGmMb\"})\n",
    "                    if not image_tag:\n",
    "                        image_tag = psoup.find(\"img\", {\"class\": \"_396cs4 _2amPTt\"})\n",
    "                    image_url = image_tag[\"src\"]\n",
    "                except:\n",
    "                    image_url = None\n",
    "                \n",
    "                if not image_url or not price:\n",
    "                    continue\n",
    "                \n",
    "                # download image\n",
    "                image_name = re.sub(r'\\W+', '_', name)[:40] + \".jpg\"\n",
    "                image_path = os.path.join(\"fashion_images\", image_name)\n",
    "                \n",
    "                try:\n",
    "                    img_data = requests.get(image_url).content\n",
    "                    with open(image_path, \"wb\") as f:\n",
    "                        f.write(img_data)\n",
    "                except:\n",
    "                    continue\n",
    "                \n",
    "                all_data.append({\n",
    "                    \"category\": query.replace(\"+\", \" \"),\n",
    "                    \"product_name\": name,\n",
    "                    \"brand\": brand,\n",
    "                    \"price\": price,\n",
    "                    \"rating\": rating,\n",
    "                    \"description\": description,\n",
    "                    \"image_name\": image_name,\n",
    "                    \"image_url\": image_url,\n",
    "                    \"product_link\": product_url\n",
    "                })\n",
    "                \n",
    "                print(f\"‚úÖ Saved: {name[:40]}\")\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(\"‚ùå Error:\", e)\n",
    "                continue\n",
    "\n",
    "# üì¶ Save final dataset\n",
    "df = pd.DataFrame(all_data)\n",
    "df.to_csv(\"fashion_dataset_full.csv\", index=False)\n",
    "print(\"\\n‚úÖ ALL DONE!\")\n",
    "print(f\"Total products scraped: {len(df)}\")\n",
    "print(\"Saved as fashion_dataset_full.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ce56a1f-0e6f-47fc-be86-82bc723991c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
